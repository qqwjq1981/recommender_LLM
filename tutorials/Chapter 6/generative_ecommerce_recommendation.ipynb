{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import azureml.core\n",
        "from azureml.core import Workspace, Datastore\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "from azure.ai.ml import load_component\n",
        "from azure.ai.ml.entities import Environment\n",
        "from datetime import datetime, timedelta"
      ],
      "outputs": [],
      "execution_count": 393,
      "metadata": {
        "gather": {
          "logged": 1725360917736
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the YAML file\n",
        "with open('./api.yaml', 'r') as yaml_file:\n",
        "    data = yaml.safe_load(yaml_file)\n",
        "\n",
        "# Access the API keys and other configuration data\n",
        "weaviate_url = data.get('weaviate').get('url')\n",
        "weaviate_api_key = data.get('weaviate').get('api_key')\n",
        "cohere_api_key = data.get('cohere').get('api_key')\n",
        "openai_api_key = data.get('openai').get('api_key')\n",
        "serper_api_key = data.get('serper').get('api_key')\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "os.environ[\"SERPER_API_KEY\"] = serper_api_key\n",
        "SUBSCRIPTION = data.get('azure').get('subscription_id')\n",
        "RESOURCE_GROUP = data.get('azure').get('resource_group_name')\n",
        "WS_NAME = data.get('azure').get('workspace_name')"
      ],
      "outputs": [],
      "execution_count": 394,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725360917889
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\n",
        "\n",
        "# authenticate\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=SUBSCRIPTION,\n",
        "    resource_group_name=RESOURCE_GROUP,\n",
        "    workspace_name=WS_NAME,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 395,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725360918807
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_dir = \"./feeds_pipeline\"\n",
        "os.makedirs(pipeline_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 396,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725360919037
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {pipeline_dir}/conda.yaml\n",
        "name: model-env\n",
        "channels:\n",
        "    - conda-forge\n",
        "    - anaconda\n",
        "dependencies:\n",
        "    - python=3.10\n",
        "    - pip:\n",
        "        - mlflow\n",
        "        - crewai\n",
        "        - langchain   #==0.2.5\n",
        "        - langchain_openai   #==0.1.8\n",
        "        - matplotlib\n",
        "        - openai    #==1.35.13\n",
        "        - plotly\n",
        "        - PyYAML\n",
        "        - numpy\n",
        "        - scikit-learn\n",
        "        - scipy\n",
        "        - pandas\n",
        "        - tiktoken\n",
        "        - unstructured\n",
        "        - weaviate-client\n",
        "        - azureml-mlflow\n",
        "        - inference-schema[numpy-support]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./feeds_pipeline/conda.yaml\n"
        }
      ],
      "execution_count": 397,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom_env_name = \"linkedin-learn\"\n",
        "\n",
        "# # pipeline_job_env=Environment.from_pip_requirements(custom_env_name, '../../../../test_script/requirements.txt')\n",
        "# pipeline_job_env = Environment(\n",
        "#     name=custom_env_name,\n",
        "#     description=\"Custom environment for newsletter Defaults pipeline\",\n",
        "#     conda_file=os.path.join(pipeline_dir, \"conda.yaml\"),\n",
        "#     image=\"mcr.microsoft.com/azureml/curated/sklearn-1.5:2\",\n",
        "# )\n",
        "# pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "# print(\n",
        "#     f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        "# )"
      ],
      "outputs": [],
      "execution_count": 398,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725360919306
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_models_src_dir = \"./feeds_components/ml_models\"\n",
        "os.makedirs(ml_models_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 399,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725360919480
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from azure.ai.ml.entities import Data\n",
        "# from azure.ai.ml.constants import AssetTypes\n",
        "# import ast\n",
        "# import json\n",
        "# data_asset = ml_client.data.get(name=\"linkedin_profile\", version=v1)\n",
        "# print(f\"Data asset URI: {data_asset.path}\")\n",
        "\n",
        "# # read into pandas - note that you will see 2 headers in your data frame - that is ok, for now\n",
        "\n",
        "# data = pd.read_csv(data_asset.path)\n",
        "# def parse_literal(column_value):\n",
        "#     try:\n",
        "#         return ast.literal_eval(column_value)\n",
        "#     except (ValueError, SyntaxError):\n",
        "#         return column_value\n",
        "\n",
        "# profile_dict = {}\n",
        "\n",
        "# for index, row in data.iterrows():\n",
        "#     row_dict = row.to_dict()\n",
        "#     for key, value in row_dict.items():\n",
        "#         if isinstance(value, str) and (value.startswith('{') or value.startswith('[')):\n",
        "#             row_dict[key] = parse_literal(value)\n",
        "\n",
        "#         # break\n",
        "#     text_str = json.dumps(row_dict)\n",
        "#     profile_dict[f'person_{index+1}'] = text_str\n",
        "# list(profile_dict.values())"
      ],
      "outputs": [],
      "execution_count": 400,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725360919698
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {ml_models_src_dir}/interest_and_topic.py\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import ast\n",
        "import logging\n",
        "import mlflow\n",
        "import argparse\n",
        "\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain import OpenAI\n",
        "from weaviate.classes.query import HybridFusion\n",
        "from weaviate.classes.query import MetadataQuery\n",
        "\n",
        "\n",
        "def parse_literal(column_value):\n",
        "    try:\n",
        "        return ast.literal_eval(column_value)\n",
        "    except (ValueError, SyntaxError):\n",
        "        return column_value\n",
        "\n",
        "def main():\n",
        "    \n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--profile\", type=str, help=\"path to linkedin profile\")\n",
        "    parser.add_argument(\"--data_folder\", type=str, help=\"path to data folder\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    input_path = args.profile\n",
        "    loader = CSVLoader(input_path,csv_args={\n",
        "        'delimiter': '\\t'})\n",
        "    data = loader.load()\n",
        "\n",
        "    llm = OpenAI(max_tokens=1500)\n",
        "\n",
        "    chain = load_summarize_chain(llm, chain_type=\"refine\", verbose=True)\n",
        "\n",
        "    understanding_result = chain.invoke(data)[\"output_text\"]\n",
        "        \n",
        "    interests = []\n",
        "    topics=[]\n",
        "    profiles = understanding_result\n",
        "    for summary in understanding_result:\n",
        "        prompt = (f\"Given the following profile information, summarize five key personal interests as keywords (which may involves this person's ) and five potential topics the person might want to learn more about.\\n\"\n",
        "                f\"profile: ###\\n {summary}\\n ###\\n\"\n",
        "                f\"Desired format:\\n\"\n",
        "                f\"Interest Keywords:\\n\"\n",
        "                f\"1. \\n 2. \\n 3. \\n 4. \\n 5. \\n\"\n",
        "                f\"Topics:\\n\"\n",
        "                f\"1. \\n 2. \\n 3. \\n 4. \\n 5. \\n\"\n",
        "                f\"For example:\"\n",
        "                f\"profile:###\\n {summary}\\n ###\\n\"\n",
        "                f\"Interest Keywords:\\n\"\n",
        "                f\"1. \\n 2. \\n 3. \\n 4. \\n 5. \\n\"\n",
        "                f\"Topics:\\n\"\n",
        "                f\"1. \\n 2. \\n 3. \\n 4. \\n 5. \\n\"\n",
        "                )\n",
        "\n",
        "        result = llm.invoke(prompt)\n",
        "\n",
        "        result_list = result.split('\\n')\n",
        "        interests.append(\"#\".join(result_list[2:7]))\n",
        "        topics.append(\"#\".join(result_list[8:13]))\n",
        "    data[\"interest\"] = interest\n",
        "    data[\"topics\"] = topics\n",
        "    df.to_csv(os.path.join(args.data_folder, \"interest_and_topic.csv\"), index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./feeds_components/ml_models/interest_and_topic.py\n"
        }
      ],
      "execution_count": 401,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723535758667
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {ml_models_src_dir}/interest_and_topic.yml\n",
        "# <component>\n",
        "name: interest_and_topic\n",
        "display_name: predict interest and topic based on profile\n",
        "# version: 1 # Not specifying a version will automatically update the version\n",
        "type: command\n",
        "inputs:\n",
        "  profile: \n",
        "    type: uri_file\n",
        "outputs:\n",
        "  data_folder: \n",
        "    type: uri_folder\n",
        "code: .\n",
        "environment:\n",
        "  azureml:linkedin-learn:2\n",
        "command: >-\n",
        "  python interest_and_topic.py \n",
        "  --profile ${{inputs.profile}} \n",
        "  --data_folder ${{outputs.data_folder}}\n",
        "# </component>"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./feeds_components/ml_models/interest_and_topic.yml\n"
        }
      ],
      "execution_count": 402,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the component from the yml file\n",
        "interest_and_topic_component = load_component(source=os.path.join(ml_models_src_dir, \"interest_and_topic.yml\"))\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "interest_and_topic_component = ml_client.create_or_update(interest_and_topic_component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {interest_and_topic_component.name} with Version {interest_and_topic_component.version} is registered\"\n",
        ")\n",
        "\n",
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "data_asset = ml_client.data.get(name=\"linkedin_profile\", version='1')\n",
        "print(f\"Data asset URI: {data_asset.path}\")\n",
        "\n",
        "url_folder = ml_client.data.get(\"newsletter_v1\", version=\"1\").path\n",
        "data_folder = Output(type=\"uri_folder\", path=url_folder,mode=\"rw_mount\")\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n",
        "    description=\"profile analysis\",\n",
        ")\n",
        "def profile_analysis_pipeline(\n",
        "    pipeline_job_profile,\n",
        "):\n",
        "    # using data_prep_function like a python call with its own inputs\n",
        "    interest_and_topic_job = interest_and_topic_component(\n",
        "        profile=pipeline_job_profile,\n",
        "    )\n",
        "\n",
        "    # example how to change path of output on pipeline level\n",
        "    interest_and_topic_job.outputs.data_folder = data_folder\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"JSON_FORMAT_curation\": interest_and_topic_job.outputs.data_folder,\n",
        "    }\n",
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = profile_analysis_pipeline(\n",
        "    pipeline_job_profile=Input(type=\"uri_file\", path=data_asset.path),\n",
        ")\n",
        "\n",
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"linkedin-learn-exp-1\",\n",
        ")\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component interest_and_topic with Version 2024-09-03-10-55-19-2149333 is registered\nData asset URI: azureml://subscriptions/541beb67-718e-41c5-958e-8cc0ba95b210/resourcegroups/awesome_rag_dev/workspaces/rag_book_demo/datastores/workspaceblobstore/paths/UI/2024-09-03_083318_UTC/LinkedIn_Dataset.tsv\nRunId: cyan_fly_nyppgm56gc\nWeb View: https://ml.azure.com/runs/cyan_fly_nyppgm56gc?wsid=/subscriptions/541beb67-718e-41c5-958e-8cc0ba95b210/resourcegroups/awesome_rag_dev/workspaces/rag_book_demo\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-09-03 10:55:24Z] Submitting 1 runs, first five are: 314c4a12:d63d72d2-12af-4110-9b42-e1e51ce7e9ad\n[2024-09-03 11:01:49Z] Execution of experiment failed, update experiment status and cancel running nodes.\n\nExecution Summary\n=================\nRunId: cyan_fly_nyppgm56gc\nWeb View: https://ml.azure.com/runs/cyan_fly_nyppgm56gc?wsid=/subscriptions/541beb67-718e-41c5-958e-8cc0ba95b210/resourcegroups/awesome_rag_dev/workspaces/rag_book_demo\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
        },
        {
          "output_type": "error",
          "ename": "JobException",
          "evalue": "Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /interest_and_topic_job. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"eastus\",\n    \"location\": \"eastus\",\n    \"time\": \"2024-09-03T11:01:49.500025Z\",\n    \"component_name\": \"\"\n} ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJobException\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[403], line 54\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# submit the pipeline job\u001b[39;00m\n\u001b[1;32m     49\u001b[0m pipeline_job \u001b[38;5;241m=\u001b[39m ml_client\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mcreate_or_update(\n\u001b[1;32m     50\u001b[0m     pipeline,\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Project's name\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     experiment_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinkedin-learn-exp-1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     53\u001b[0m )\n\u001b[0;32m---> 54\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:94\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:289\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39mspan():\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[1;32m    287\u001b[0m             logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[1;32m    288\u001b[0m         ):\n\u001b[0;32m--> 289\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage_logger\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:818\u001b[0m, in \u001b[0;36mJobOperations.stream\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineChildJobError(job_id\u001b[38;5;241m=\u001b[39mjob_object\u001b[38;5;241m.\u001b[39mid)\n\u001b[0;32m--> 818\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_logs_until_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runs_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datastore_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequests_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requests_pipeline\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_ops_helper.py:334\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[0;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[1;32m    332\u001b[0m         file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m JobException(\n\u001b[1;32m    335\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(json\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)),\n\u001b[1;32m    336\u001b[0m             target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mJOB,\n\u001b[1;32m    337\u001b[0m             no_personal_data_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException raised on failed job.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    338\u001b[0m             error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mSYSTEM_ERROR,\n\u001b[1;32m    339\u001b[0m         )\n\u001b[1;32m    341\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    342\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mflush()\n",
            "\u001b[0;31mJobException\u001b[0m: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /interest_and_topic_job. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"eastus\",\n    \"location\": \"eastus\",\n    \"time\": \"2024-09-03T11:01:49.500025Z\",\n    \"component_name\": \"\"\n} "
          ]
        }
      ],
      "execution_count": 403,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725361363019
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################### pipeline of personal feed is to be continued################\n",
        "\n",
        "# # Connect to your Weaviate instance\n",
        "# import weaviate\n",
        "# from weaviate.classes.init import Auth\n",
        "# import os\n",
        "\n",
        "# # Connect to your Weaviate instance\n",
        "\n",
        "# # Check if your instance is live and ready\n",
        "# # This should return `True`\n",
        "# client.is_ready()\n",
        "\n",
        "# collection = client.collections.get(\"Article\")\n",
        "\n",
        "# def hybrid_query_weaviate(query, collection_name, alpha_val):\n",
        "    \n",
        "#     collection = client.collections.get(collection_name)\n",
        "\n",
        "#     response = collection.query.hybrid(\n",
        "#         query=query, \n",
        "#         alpha=alpha_val, \n",
        "#         limit=5,\n",
        "#         return_metadata=MetadataQuery(score=True),\n",
        "#         fusion_type=HybridFusion.RANKED\n",
        "#     )\n",
        "\n",
        "#     return response.objects\n",
        "\n",
        "# print(\"Personalized Feeds:\\n\")\n",
        "# output_file_path = 'personalized_feeds.txt'\n",
        "# with open(output_file_path, 'w') as file:\n",
        "#     file.write(\"Personalized Feeds:\\n\\n\")\n",
        "\n",
        "#     for interest in interests_list[2:7]:\n",
        "        \n",
        "#         query_result = hybrid_query_weaviate(interest, \"Article\", 0.5)\n",
        "#         print(f\"'{interest}' feed:\")\n",
        "#         file.write(f\"'{interest}' feed:\\n\")\n",
        "\n",
        "#         for i, o in enumerate(query_result):\n",
        "#             article = o.properties\n",
        "#             score = o.metadata.score\n",
        "#             print(f\"({i+1}) {article['title']}:\\n {article['content']}\\n (Score: {score})\")\n",
        "#             file.write(f\"({i+1}) {article['title']}:\\n {article['content']}\\n (Score: {score})\\n\")\n",
        "#         file.write(\"----------\\n\\n\")\n",
        "#         print(\"----------\")\n",
        "#         print(\"\\n\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725360276620
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {ml_models_src_dir}/embedding.py\n",
        "import os\n",
        "import tiktoken\n",
        "import textwrap as tr\n",
        "from typing import List, Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from scipy import spatial\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "import mlflow\n",
        "import argparse\n",
        "\n",
        "\n",
        "client = OpenAI(max_retries=5)\n",
        "\n",
        "def get_embedding(text: str, model=\"text-embedding-3-small\", **kwargs) -> List[float]:\n",
        "    # replace newlines, which can negatively affect performance.\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    response = client.embeddings.create(input=[text], model=model, **kwargs)\n",
        "\n",
        "    return response.data[0].embedding\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--crawled_path\", type=str, help=\"path to crawled data\")\n",
        "    parser.add_argument(\"--max_tokens\", type=int, default=8191)\n",
        "    parser.add_argument(\"--n_top\", type=int, default=1000, help=\"number of selected news to be processed\")\n",
        "    parser.add_argument(\"--emb_data\", type=str, help=\"path to embedding data\")\n",
        "    parser.add_argument(\"--output_suffix\", type=str, help=\"suffix of output data\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    crawled_path = args.crawled_path\n",
        "    n_top = args.n_top  #1000\n",
        "    output_suffix = args.output_suffix\n",
        "    emb_data_path = args.emb_data\n",
        "\n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    index_path = os.path.join(crawled_path, \"rss_crawled_data_index.tsv\")\n",
        "    crawled_index = pd.read_csv(index_path, sep='\\t', encoding=\"latin_1\")\n",
        "\n",
        "    crawled_data = []\n",
        "    for input_suffix in crawled_index['suffix']:\n",
        "        print(input_suffix)\n",
        "        input_path = os.path.join(crawled_path, \"rss_crawled_data_\" + input_suffix + \".csv\")\n",
        "        crawled_data_cur = pd.read_csv(input_path, sep='\\t', encoding=\"latin_1\")\n",
        "        crawled_data.append(crawled_data_cur)\n",
        "\n",
        "    crawled_df = pd.concat(crawled_data, ignore_index=True)\n",
        "\n",
        "    embedding_model = \"text-embedding-3-small\"\n",
        "    embedding_encoding = \"cl100k_base\"\n",
        "    max_tokens = args.max_tokens  #8191  # the maximum for text-embedding-3-small is 8191\n",
        "\n",
        "    crawled_df['title_summary'] = crawled_df.apply(lambda row: f\"{row['title']}; {row['summary']}\", axis=1)\n",
        "\n",
        "    crawled_df[\"title_summary\"] = crawled_df[\"title_summary\"].str.slice(0,max_tokens).to_frame()\n",
        "\n",
        "    crawled_df = crawled_df.drop_duplicates(subset=[\"title_summary\"],ignore_index=False)\n",
        "    #select the top n news from corpus\n",
        "    # crawled_df = crawled_df.tail(n_top)\n",
        "    #tokenize the news\n",
        "    encoding = tiktoken.get_encoding(embedding_encoding)\n",
        "    \n",
        "    crawled_df[\"n_tokens\"] = crawled_df.title_summary.apply(lambda x: len(encoding.encode(x)))\n",
        "    \n",
        "    crawled_df[\"embedding\"] = crawled_df.title_summary.apply(lambda x: get_embedding(x, model=embedding_model))\n",
        "\n",
        "    # if not os.path.exists(emb_data_path):\n",
        "    #     os.makedirs(emb_data_path)\n",
        "\n",
        "    output_path = os.path.join(args.emb_data, \"crawl_data_emb_\" + output_suffix + \".csv\")\n",
        "    crawled_df.to_csv(output_path, sep = '\\t', index=False)\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {ml_models_src_dir}/embedding.yml\n",
        "# <component>\n",
        "name: embedding_news_defaults_model\n",
        "display_name: embedding news Defaults Model\n",
        "# version: 1 # Not specifying a version will automatically update the version\n",
        "type: command\n",
        "inputs:\n",
        "  crawled_path:\n",
        "    type: uri_folder\n",
        "  max_tokens:\n",
        "    type: number\n",
        "  n_top:\n",
        "    type: number\n",
        "  output_suffix:\n",
        "    type: string\n",
        "outputs:\n",
        "  emb_data:\n",
        "    type: uri_folder\n",
        "code: .\n",
        "environment:\n",
        "  azureml:linkedin-learn:1\n",
        "command: >-\n",
        "  python embedding.py \n",
        "  --crawled_path ${{inputs.crawled_path}} \n",
        "  --max_tokens ${{inputs.max_tokens}} \n",
        "  --n_top ${{inputs.n_top}}\n",
        "  --emb_data ${{outputs.emb_data}}\n",
        "  --output_suffix ${{inputs.output_suffix}}\n",
        "# </component>"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the component from the yml file\n",
        "embedding_component = load_component(source=os.path.join(ml_models_src_dir, \"embedding.yml\"))\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "embedding_component = ml_client.create_or_update(embedding_component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {embedding_component.name} with Version {embedding_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725360276647
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {ml_models_src_dir}/cluster.py\n",
        "import argparse\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "import os\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "from ast import literal_eval\n",
        "import pandas.core.strings as pd_strings  # Import for string manipulation\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "def join_vector_as_string(vector, separator=\"\"):\n",
        "  \"\"\"Joins a vector of integers as a string using the provided separator.\n",
        "  Args:\n",
        "    vector: A list or numpy array of integers.\n",
        "    separator: The string to use between elements (default is comma).\n",
        "  Returns:\n",
        "    A string containing the joined elements.\n",
        "  \"\"\"\n",
        "  string_vector = [str(v) for v in vector]  # Convert elements to strings using list comprehension\n",
        "  joined_string = separator.join(string_vector)\n",
        "  return joined_string\n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "    Args:\n",
        "        path (str): path to directory or file to choose\n",
        "    Returns:\n",
        "        str: full path of selected file\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "# Start Logging\n",
        "mlflow.start_run()\n",
        "\n",
        "# enable autologging\n",
        "mlflow.sklearn.autolog()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--emb_data\", type=str, help=\"path to embedding data\")\n",
        "    parser.add_argument(\"--input_suffix\", type=str, help=\"suffix of the input data file\")\n",
        "    parser.add_argument(\"--n_components\", required=False, default=25, type=int)\n",
        "    parser.add_argument(\"--random_state\", required=False, default=42, type=int)\n",
        "    parser.add_argument(\"--cluster_data\", type=str, help=\"path to group cluster\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    input_suffix = args.input_suffix\n",
        "\n",
        "    input_path = os.path.join(args.emb_data, \"crawl_data_emb_\" + input_suffix + \".csv\")\n",
        "    emb_df = pd.read_csv(input_path, sep = '\\t')\n",
        "    \n",
        "    emb_df[\"embedding\"] = emb_df.embedding.apply(literal_eval).apply(np.array)\n",
        "    emb = np.stack(emb_df[\"embedding\"])\n",
        "\n",
        "    transformer = GaussianRandomProjection(n_components=25, random_state=42)\n",
        "    projected_embeddings = pd.DataFrame(transformer.fit_transform(emb))\n",
        "    df_transformed = projected_embeddings.gt(0).astype(int)\n",
        "    df_transformed['cluster_id'] = df_transformed.apply(lambda x: join_vector_as_string(x), axis=1)\n",
        "\n",
        "    emb_df['cluster_id'] = df_transformed['cluster_id']\n",
        "\n",
        "    output_path = os.path.join(args.cluster_data, \"cluster_data_\" + input_suffix + \".csv\")\n",
        "    emb_df.to_csv(output_path, sep='\\t', index=False)\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {ml_models_src_dir}/cluster.yml\n",
        "# <component>\n",
        "name: cluster_news_defaults_model\n",
        "display_name: cluster news Defaults Model\n",
        "type: command\n",
        "inputs:\n",
        "  emb_data: \n",
        "    type: uri_folder\n",
        "  n_components:\n",
        "    type: number\n",
        "  random_state:\n",
        "    type: number\n",
        "  input_suffix:\n",
        "    type: string\n",
        "outputs:\n",
        "  cluster_data:\n",
        "    type: uri_folder\n",
        "code: .\n",
        "environment:\n",
        "  azureml:linkedin-learn:1\n",
        "command: >-\n",
        "  python cluster.py \n",
        "  --emb_data ${{inputs.emb_data}}\n",
        "  --n_components ${{inputs.n_components}}\n",
        "  --random_state ${{inputs.random_state}}\n",
        "  --input_suffix ${{inputs.input_suffix}}\n",
        "  --cluster_data ${{outputs.cluster_data}}\n",
        "# </component>"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the component from the yml file\n",
        "cluster_component = load_component(source=os.path.join(ml_models_src_dir, \"cluster.yml\"))\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "cluster_component = ml_client.create_or_update(cluster_component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {cluster_component.name} with Version {cluster_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725360276675
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile {ml_models_src_dir}/curator.py\n",
        "# import argparse\n",
        "# import os\n",
        "# import pandas as pd\n",
        "# from ast import literal_eval\n",
        "# import mlflow\n",
        "# import numpy as np\n",
        "# from langchain.agents import Tool\n",
        "# from crewai import Agent, Task, Process, Crew\n",
        "# from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "# from langchain_openai import OpenAI\n",
        "# import json\n",
        "# import smtplib\n",
        "# from email.mime.text import MIMEText\n",
        "# from email.mime.multipart import MIMEMultipart\n",
        "\n",
        "\n",
        "# def send_email(recipient_email,subject = \"Test Email from Python\", message = \"This is a test email sent using Python's smtplib library.\"):\n",
        "\n",
        "#     # Create a secure connection with SMTP server (replace with your SMTP server details)\n",
        "#     with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:\n",
        "\n",
        "#         # Example usage (replace placeholders with your details)\n",
        "#         server.login(sender_email, sender_password)\n",
        "#         # Create a MIMEMultipart message for better formatting (optional, but recommended)\n",
        "#         msg = MIMEMultipart()\n",
        "#         msg['From'] = sender_email\n",
        "#         msg['To'] = recipient_email\n",
        "#         msg['Subject'] = subject\n",
        "\n",
        "#         # Set message content as plain text\n",
        "#         msg.attach(MIMEText(message, 'plain'))  # You can also use 'html' for HTML content\n",
        "\n",
        "#         # Send the email\n",
        "#         server.sendmail(sender_email, recipient_email, msg.as_string())\n",
        "#         print('Email sent successfully!')\n",
        "\n",
        "# def join_vector_as_string(vector, separator=\"\"):\n",
        "#   \"\"\"Joins a vector of integers as a string using the provided separator.\n",
        "#   Args:\n",
        "#     vector: A list or numpy array of integers.\n",
        "#     separator: The string to use between elements (default is comma).\n",
        "#   Returns:\n",
        "#     A string containing the joined elements.\n",
        "#   \"\"\"\n",
        "#   string_vector = [str(v) for v in vector]  # Convert elements to strings using list comprehension\n",
        "#   joined_string = separator.join(string_vector)\n",
        "#   return joined_string\n",
        "\n",
        "# def select_first_file(path):\n",
        "#     \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "#     Args:\n",
        "#         path (str): path to directory or file to choose\n",
        "#     Returns:\n",
        "#         str: full path of selected file\n",
        "#     \"\"\"\n",
        "#     files = os.listdir(path)\n",
        "#     return os.path.join(path, files[0])\n",
        "\n",
        "\n",
        "# def main():\n",
        "#     \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "#     # input and output arguments\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument(\"--clean_data\", type=str, help=\"path to clean data\")\n",
        "#     parser.add_argument(\"--cluster_id\", type=str, help=\"path to group cluster\")\n",
        "#     parser.add_argument(\"--curation\", type=str, help=\"path to curated newsletters\")\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     search = GoogleSerperAPIWrapper()\n",
        "\n",
        "#     search_tool = Tool(\n",
        "#         name=\"Scrape google searches\",\n",
        "#         func=search.run,\n",
        "#         description=\"useful for when you need to ask the agent to search the internet\",\n",
        "#     )\n",
        "\n",
        "#     # To Load GPT-4 api\n",
        "#     api = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "#     explorer = Agent(\n",
        "#         role=\"web search\",\n",
        "#         goal=\"Find and explore the top news from internet when provided one or several new sources\",\n",
        "#         backstory=\"\"\"You are and Expert strategist that knows how to spot other new sources having the same event or same topic with the provided new source. The provided new source will be given as a list of news by task description\n",
        "#         You're great at finding same news with different or exclusive opinions, aspects or details.\n",
        "#         \"\"\",\n",
        "#         verbose=True,\n",
        "#         allow_delegation=False,\n",
        "#         tools=[search_tool],\n",
        "#     )\n",
        "\n",
        "#     writer = Agent(\n",
        "#         role=\"Senior Technical Writer\",\n",
        "#         goal=\"Write a newsletter about the provided new source and searched other news\",\n",
        "#         backstory=\"\"\"You are an Expert Writer on newsletter for the main topic or event of the provided new sources. Content of different news are selected or merged and takes advantage of all collected new sources in order to provide a broader view and more complete description and detail of the event or topic in most new sources. You know how to present complicated technical terms to general audience in a \n",
        "#         fun way by using layman words. If one of the new sources contains a topic or event unrelavent to the main topic or event in all new sources, do not include the unrelavent content in the generated newsletter. ONLY use other new sources from the internet and provided new source for the newsletter.\"\"\",\n",
        "#         verbose=True,\n",
        "#         allow_delegation=True,\n",
        "#     )\n",
        "\n",
        "#     criticizer = Agent(\n",
        "#         role=\"Senior Writing criticizer\",\n",
        "#         goal=\"criticise the generated newsletter and see if any facts or stats of generated newsletter is incorrect\",\n",
        "#         backstory=\"\"\"You are the expert of criticising generated newsletter to make sure the fact is true and logical based on the original provided news and search expansion. If any fact or statistics are not consistent with the original provided news and search expansion, correct these content in newsleter and output a corrected version. Comparing the newsletter and the original provided news and search expansion, see if there's any logical or statistical error and correct these fact error in newsletter to make sure the content consistency between newsletter and provided news.\"\"\",\n",
        "#         verbose=True,\n",
        "#         allow_delegation=True,    \n",
        "#     )\n",
        "\n",
        "#     clean_data = pd.read_csv(os.path.join(args.clean_data, \"clean_data.csv\"))\n",
        "#     cluster_id = pd.read_csv(os.path.join(args.cluster_id, \"cluster_id.csv\"))\n",
        "#     try:\n",
        "#         cluster_id = cluster_id.cluster_id.apply(literal_eval).apply(np.array)\n",
        "#     except:\n",
        "#         cluster_id = cluster_id.cluster_id.apply(lambda x: [x])\n",
        "\n",
        "#     ans = []\n",
        "#     for i in cluster_id:\n",
        "#         if len(i)>=1:\n",
        "#             new = \"\"\n",
        "#             for j in i:\n",
        "#                 new+= \"url of news: \"+clean_data.iloc[j][\"Current_URL\"]+\", \"\n",
        "#             task_search = Task(\n",
        "#                 description=\"\"\"search internet for the top news with the provided new source as follows\"\"\"+new,\n",
        "#                 agent=explorer,\n",
        "#                 expected_output=\"\"\"\n",
        "#                 For your Outputs use the following markdown format:\n",
        "#                 ## [rank][Title of the new](link to new)\n",
        "#                 - news content\n",
        "#                 \"\"\",\n",
        "#             )\n",
        "\n",
        "#             task_newsletter = Task(\n",
        "#                 description=\"\"\"Write a newsletter with text only and with a short but impactful headline and at least 20 paragraphs.\n",
        "#                 \"\"\",\n",
        "#                 agent=writer,\n",
        "#                 expected_output=\"\"\"\n",
        "#                 For your Outputs use the following markdown format:\n",
        "#                 ## [Title of newsletter](link to project)\n",
        "#                 - Interesting facts\n",
        "#                 \"\"\",\n",
        "#             )\n",
        "\n",
        "#             task_revise = Task(\n",
        "#                 description=\"\"\"compare the generated newsletter and provided news, correct the corresponding content of newsletter and make sure there's no statistical, logical error and fact. \n",
        "#                 \"\"\",\n",
        "#                 agent=criticizer,\n",
        "#                 expected_output=\"\"\"For your Outputs use the following markdown format:\n",
        "#                 ## [Title of newsletter]\n",
        "#                 - Interesting facts\n",
        "#                 \"\"\"\n",
        "#             )\n",
        "\n",
        "#             # instantiate crew of agents\n",
        "#             crew = Crew(\n",
        "#                 agents=[explorer, writer, criticizer],\n",
        "#                 tasks=[task_search, task_newsletter, task_revise],\n",
        "#                 verbose=True,\n",
        "#                 process=Process.sequential,  # Sequential process will have tasks executed one after the other and the outcome of the previous one is passed as extra content into this next.\n",
        "#                 planning=True,\n",
        "#                 planning_llm=OpenAI(model=\"gpt-4o\")\n",
        "#             )\n",
        "            \n",
        "#             # Get your crew to work!\n",
        "#             result = crew.kickoff()\n",
        "#             send_email(\"wuyifu2f@gmail.com\", result.raw.split(\"\\n\")[0] , result.raw)\n",
        "#             ans.append(json.dumps(result.json_dict))\n",
        "\n",
        "#     ans = pd.DataFrame(ans, columns=['curation'],dtype='string')\n",
        "#     ans.to_csv(os.path.join(args.curation, \"curation.csv\"), index=False)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725360276695
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile {ml_models_src_dir}/curator.yml\n",
        "# # <component>\n",
        "# name: curate_newsletter_defaults_model\n",
        "# display_name: curate newsletter Defaults Model\n",
        "# # version: 1 # Not specifying a version will automatically update the version\n",
        "# type: command\n",
        "# inputs:\n",
        "#   clean_data: \n",
        "#     type: uri_folder\n",
        "#   cluster_id:\n",
        "#     type: uri_folder\n",
        "# outputs:\n",
        "#   curation: \n",
        "#     type: uri_folder\n",
        "# code: .\n",
        "# environment:\n",
        "#   # for this step, we'll use an AzureML curate environment\n",
        "#   # azureml://registries/azureml/environments/sklearn-1.0/labels/latest\n",
        "#   # azureml://locations/eastus/workspaces/ca94b34d-7f1c-4e88-bc05-341b8e447df2/environments/newsletter-learn/versions/29\n",
        "#   azureml:newsletter-learn:30\n",
        "# command: >-\n",
        "#   python curator.py \n",
        "#   --clean_data ${{inputs.clean_data}}\n",
        "#   --cluster_id ${{inputs.cluster_id}} \n",
        "#   --curation ${{outputs.curation}}\n",
        "# # </component>"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725360276715
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Loading the component from the yml file\n",
        "# curator_component = load_component(source=os.path.join(ml_models_src_dir, \"curator.yml\"))\n",
        "\n",
        "# # Now we register the component to the workspace\n",
        "# curator_component = ml_client.create_or_update(curator_component)\n",
        "\n",
        "# # Create (register) the component in your workspace\n",
        "# print(\n",
        "#     f\"Component {curator_component.name} with Version {curator_component.version} is registered\"\n",
        "# )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725360276731
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "# url_folder = \"azureml://subscriptions/541beb67-718e-41c5-958e-8cc0ba95b210/resourcegroups/awesome_rag_dev/workspaces/rag_book_demo/datastores/workspaceblobstore/paths/UI/2024-07-27_231028_UTC/\"\n",
        "url_folder = ml_client.data.get(\"newsletter_v1\", version=\"1\").path\n",
        "\n",
        "emb_data = Output(type=\"uri_folder\", path=url_folder,mode=\"rw_mount\")\n",
        "cluster_data = Output(type=\"uri_folder\", path=url_folder,mode=\"rw_mount\")\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n",
        "    description=\"newsletter curation pipeline\",\n",
        ")\n",
        "\n",
        "def newsletter_defaults_pipeline(\n",
        "    pipeline_job_data_input,\n",
        "    pipeline_job_max_tokens,\n",
        "    pipeline_job_n_top,\n",
        "    pipeline_job_n_components,\n",
        "    pipeline_job_random_state,\n",
        "):\n",
        "    output_suffix = datetime.now().strftime(\"%Y-%m-%dT%H\")\n",
        "\n",
        "    # using data_prep_function like a python call with its own inputs\n",
        "    embedding_job = embedding_component(\n",
        "        crawled_path=pipeline_job_data_input,\n",
        "        max_tokens=pipeline_job_max_tokens,\n",
        "        n_top=pipeline_job_n_top,\n",
        "        output_suffix = output_suffix,\n",
        "    )\n",
        "\n",
        "    # using cluster_func like a python call with its own inputs\n",
        "    cluster_job = cluster_component(\n",
        "        emb_data=embedding_job.outputs.emb_data,  # note: using outputs from previous step\n",
        "        n_components=pipeline_job_n_components,\n",
        "        random_state=pipeline_job_random_state,\n",
        "        input_suffix=output_suffix,\n",
        "    )\n",
        "\n",
        "    # example how to change path of output on pipeline level\n",
        "    embedding_job.outputs.emb_data = emb_data\n",
        "    cluster_job.outputs.cluster_data = cluster_data\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"JSON_FORMAT_curation\": cluster_job.outputs.cluster_data,\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725360276749
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model_name = \"newsletter_defaults_model\"\n",
        "\n",
        "# get a handle of the data asset and print the URI\n",
        "crawled_data = ml_client.data.get(name=\"rss_crawl\", version=\"3\")\n",
        "\n",
        "print(f\"Data asset URI: {crawled_data.path}\")\n",
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = newsletter_defaults_pipeline(\n",
        "    pipeline_job_data_input=Input(type=\"uri_folder\", path=crawled_data.path),\n",
        "    pipeline_job_max_tokens=8191,\n",
        "    pipeline_job_n_top=1000,\n",
        "    pipeline_job_n_components=25,\n",
        "    pipeline_job_random_state=42,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725360276773
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # submit the pipeline job\n",
        "# pipeline_job = ml_client.jobs.create_or_update(\n",
        "#     pipeline,\n",
        "#     # Project's name\n",
        "#     experiment_name=\"linkedin-learn-exp-1\",\n",
        "# )\n",
        "# ml_client.jobs.stream(pipeline_job.name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725360276792
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
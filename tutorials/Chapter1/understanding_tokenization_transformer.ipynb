{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import azureml.core\n",
        "from azureml.core import Workspace, Datastore, ComputeTarget\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "from azure.ai.ml import load_component\n",
        "from azure.ai.ml.entities import Environment, Data, PipelineJob, Job, Schedule\n",
        "from datetime import datetime, timedelta"
      ],
      "outputs": [],
      "execution_count": 150,
      "metadata": {
        "gather": {
          "logged": 1725271063777
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the YAML file\n",
        "with open('./api.yaml', 'r') as yaml_file:\n",
        "    data = yaml.safe_load(yaml_file)\n",
        "\n",
        "# Access the API keys and other configuration data\n",
        "weaviate_url = data.get('weaviate').get('url')\n",
        "weaviate_api_key = data.get('weaviate').get('api_key')\n",
        "cohere_api_key = data.get('cohere').get('api_key')\n",
        "openai_api_key = data.get('openai').get('api_key')\n",
        "serper_api_key = data.get('serper').get('api_key')\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "os.environ[\"SERPER_API_KEY\"] = serper_api_key\n",
        "SUBSCRIPTION = data.get('azure').get('subscription_id')\n",
        "RESOURCE_GROUP = data.get('azure').get('resource_group_name')\n",
        "WS_NAME = data.get('azure').get('workspace_name')"
      ],
      "outputs": [],
      "execution_count": 151,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725271063891
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\n",
        "datastore = Datastore.get(ws, \"workspaceblobstore\")\n",
        "\n",
        "# authenticate\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=SUBSCRIPTION,\n",
        "    resource_group_name=RESOURCE_GROUP,\n",
        "    workspace_name=WS_NAME,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 152,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725271064597
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an environment with Conda dependencies\n",
        "env = Environment(\n",
        "    name=\"rss-env\",\n",
        "    image=\"mcr.microsoft.com/azureml/curated/sklearn-1.5:2\",\n",
        "    conda_file={\n",
        "        \"dependencies\": [\n",
        "            \"python=3.8\",\n",
        "            {\n",
        "                \"pip\": [\n",
        "                    \"feedparser\",\n",
        "                    \"beautifulsoup4\",\n",
        "                    \"pandas\",\n",
        "                    \"swifter\",\n",
        "                     \"article-parser\"\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 153,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725271064822
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_dir = \"./rss_pipeline\"\n",
        "os.makedirs(pipeline_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 154,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725271064934
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_models_src_dir = \"./rss_components/ml_models\"\n",
        "os.makedirs(ml_models_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 155,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725271065028
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {ml_models_src_dir}/rss_fetch_feed.py\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import base64\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import re\n",
        "import swifter\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from dateutil.parser import parse\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from requests.exceptions import RequestException, ConnectionError, Timeout\n",
        "\n",
        "def parse_pubdate(pubdate_string):\n",
        "    try:\n",
        "        parsed_date = parse(pubdate_string)\n",
        "        return parsed_date\n",
        "    except ValueError:\n",
        "        print(f\"Error parsing pubDate: {pubdate_string}\")\n",
        "        return ''\n",
        "\n",
        "def get_summary_from_item(item, feed_url):\n",
        "    summary_tags = ['description', 'content']\n",
        "    for tag in summary_tags:\n",
        "        summary = item.find(tag)\n",
        "        if summary:\n",
        "            soup = BeautifulSoup(' '.join(summary.stripped_strings), 'html.parser')\n",
        "            return soup.get_text(strip=True)\n",
        "    return ''\n",
        "\n",
        "def get_url_from_item(item):\n",
        "    link = item.find('link')\n",
        "    return link.get('href') if link and link.get('href') else link.string.strip() if link and link.string else None\n",
        "    \n",
        "def get_pubDate_from_item(item):\n",
        "    default = {'pubDateRaw': '', 'pubDate': ''}\n",
        "    if item.pubDate:\n",
        "        pubDateRaw = item.pubDate.text\n",
        "        return {'pubDateRaw': pubDateRaw, 'pubDate': parse_pubdate(pubDateRaw)}\n",
        "        \n",
        "    return default\n",
        "\n",
        "def parse_rss_feed(feed_url):\n",
        "    if feed_url is None:\n",
        "        return []\n",
        "    try:\n",
        "        response = requests.get(feed_url)\n",
        "        soup = BeautifulSoup(response.content, 'xml')\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching RSS feed: {e}\")\n",
        "        return []\n",
        "\n",
        "    articles = []\n",
        "    item_tags = ['item', 'entry']\n",
        "    media_tags = ['enclosure', 'media:thumbnail', 'media:content']\n",
        "\n",
        "    for item_tag in item_tags:\n",
        "        for item in soup.find_all(item_tag):\n",
        "\n",
        "            link = get_url_from_item(item)\n",
        "            summary = get_summary_from_item(item, feed_url)\n",
        "            \n",
        "            article = {\n",
        "                'title': item.title.text,\n",
        "                'link': link,\n",
        "                'summary': summary,\n",
        "                'images': []\n",
        "            }\n",
        "            article.update(get_pubDate_from_item(item))\n",
        "\n",
        "            # Find all media elements\n",
        "            for tag in media_tags:\n",
        "                for media in item.find_all(tag):\n",
        "                    # Determine if the media is an image based on the tag\n",
        "                    is_image = (tag == 'enclosure' and media.get('type', '').startswith('image/')) or (tag in ['media:thumbnail', 'media:content'])\n",
        "                    \n",
        "                    if is_image:\n",
        "                        image_url = media.get('url')\n",
        "                        image_description = media.get('description', '')\n",
        "                        image_width = media.get('width', '')\n",
        "                        image_height = media.get('height', '')\n",
        "                        image_length = media.get('length', '')\n",
        "                        \n",
        "                        if image_url:  # Ensure the URL is present\n",
        "                            image_name = f\"{base64.urlsafe_b64encode(image_url.encode()).decode().rstrip('=')}\"\n",
        "                            max_length = 250\n",
        "                            image_name_truncate = image_name[:max_length]+ \".jpg\"\n",
        "                            article['images'].append({\n",
        "                                'url': image_url,\n",
        "                                'img_name': image_name_truncate,\n",
        "                                'description': image_description,\n",
        "                                'width': image_width,\n",
        "                                'height': image_height,\n",
        "                                'length': image_length\n",
        "                            })\n",
        "\n",
        "            articles.append(article)\n",
        "\n",
        "    return articles\n",
        "\n",
        "def save_images(articles, output_dir, output_suffix):\n",
        "    # Create the directory if it doesn't exist\n",
        "    if not os.path.exists(os.path.join(output_dir, 'images', output_suffix)):\n",
        "        os.makedirs(os.path.join(output_dir, 'images', output_suffix))\n",
        "\n",
        "    for article in articles:\n",
        "        for image in article['images']:\n",
        "            image_url = image['url']\n",
        "            image_filename = os.path.join(output_dir, 'images', output_suffix, image['img_name'])\n",
        "\n",
        "            try:\n",
        "                response = requests.get(image_url)\n",
        "                with open(image_filename, 'wb') as file:\n",
        "                    file.write(response.content)\n",
        "                print(f\"Saved image: {image_filename}\")\n",
        "            except requests.RequestException as e:\n",
        "                print(f\"Error downloading image {image_url}: {e}\")\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--feed_url\", type=str, help=\"path to feed url\")\n",
        "    parser.add_argument(\"--crawled_path\", type=str, help=\"path to crawled data\")\n",
        "    parser.add_argument(\"--output_suffix\", type=str, help=\"suffix of the output file and folder\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    input_path = os.path.join(args.feed_url, \"topic_websites_feed_url.tsv\")\n",
        "    output_suffix = args.output_suffix\n",
        "    output_path = os.path.join(args.crawled_path, \"rss_crawled_data_\" + output_suffix + \".csv\")\n",
        "\n",
        "    topic_websites = pd.read_csv(input_path, sep='\\t', encoding=\"latin_1\")\n",
        "    topic_websites = topic_websites.drop_duplicates(subset='feed_url')\n",
        "\n",
        "    articles_list = topic_websites['feed_url'].swifter.apply(parse_rss_feed)\n",
        "    \n",
        "    article_flattened = [item for sublist in articles_list for item in sublist]\n",
        "\n",
        "    df = pd.DataFrame(article_flattened)\n",
        "    df['dateTime'] = output_suffix\n",
        "    df.to_csv(output_path, sep = '\\t', index=False)\n",
        "    print('RSS data saved successfully.')\n",
        "\n",
        "    articles_list.swifter.apply(save_images, output_dir = args.crawled_path, output_suffix = output_suffix)\n",
        "\n",
        "    index_path = os.path.join(args.crawled_path, \"rss_crawled_data_index.tsv\")\n",
        "    crawled_index = pd.read_csv(index_path, sep='\\t', encoding=\"latin_1\")\n",
        "    crawled_index.loc[len(crawled_index)] = {'suffix': output_suffix}\n",
        "    crawled_index.to_csv(index_path, index=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./rss_components/ml_models/rss_fetch_feed.py\n"
        }
      ],
      "execution_count": 156,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723535758667
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {ml_models_src_dir}/rss_fetch_feed.yml\n",
        "# <component>\n",
        "name: fetch_feed_topic\n",
        "display_name: fetch and save rss feed data\n",
        "type: command\n",
        "inputs:\n",
        "  feed_url:\n",
        "    type: uri_folder\n",
        "  output_suffix:\n",
        "    type: string\n",
        "outputs:\n",
        "  crawled_path:\n",
        "    type: uri_folder\n",
        "code: .\n",
        "environment:\n",
        "  azureml:rss-env:12\n",
        "compute:\n",
        "  azureml:qqwjq99161\n",
        "command: >-\n",
        "  python rss_fetch_feed.py \n",
        "  --feed_url ${{inputs.feed_url}}\n",
        "  --output_suffix ${{inputs.output_suffix}}\n",
        "  --crawled_path ${{outputs.crawled_path}}\n",
        "# </component>"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./rss_components/ml_models/rss_fetch_feed.yml\n"
        }
      ],
      "execution_count": 157,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_instance = ComputeTarget(workspace=ws, name=\"qqwjq99161\")\n",
        "\n",
        "if compute_instance.get_status().state != 'Running':\n",
        "    compute_instance.start(wait_for_completion=True)\n",
        "\n",
        "# Loading the component from the yml file\n",
        "rss_fetch_feed_component = load_component(source=os.path.join(ml_models_src_dir, \"rss_fetch_feed.yml\"))\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "rss_fetch_feed_component = ml_client.create_or_update(rss_fetch_feed_component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {rss_fetch_feed_component.name} with Version {rss_fetch_feed_component.version} is registered\"\n",
        ")\n",
        "\n",
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "crawl_path = ml_client.data.get(\"rss_crawl\", version=\"3\").path\n",
        "input_data = Input(type='uri_folder', path = crawl_path)\n",
        "output_data = Output(type=\"uri_folder\", path=crawl_path, mode=\"rw_mount\")\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n",
        "    description=\"fetch rss feed\",\n",
        ")\n",
        "\n",
        "def rss_fetch_feed_pipeline(pipeline_feed_url):\n",
        "\n",
        "    # Format the time as YYYY-MM-DD-HH\n",
        "    output_suffix = datetime.now().strftime(\"%Y-%m-%dT%H\")\n",
        "\n",
        "    # using data_prep_function like a python call with its own inputs\n",
        "    rss_fetch_feed_job = rss_fetch_feed_component(feed_url = pipeline_feed_url, output_suffix = output_suffix)\n",
        "    rss_fetch_feed_job.allow_reuse = False  # Disable caching for this step\n",
        "\n",
        "    rss_fetch_feed_job.outputs.crawled_path = output_data\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"JSON_FORMAT_curation\": rss_fetch_feed_job.outputs.crawled_path,\n",
        "    }"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading ml_models (0.01 MBs):   0%|          | 0/7524 [00:00<?, ?it/s]\r\u001b[32mUploading ml_models (0.01 MBs): 100%|██████████| 7524/7524 [00:00<00:00, 140725.36it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component fetch_feed_topic with Version 2024-09-02-09-57-46-8606825 is registered\n"
        }
      ],
      "execution_count": 158,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725271069594
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = rss_fetch_feed_pipeline(input_data)\n",
        "\n",
        "pipeline.allow_reuse = False\n",
        "\n",
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"rss-crawl-exp-1\"\n",
        ")\n",
        "\n",
        "ml_client.jobs.stream(pipeline_job.name)\n",
        "\n",
        "# # Define the schedule parameters\n",
        "# schedule = Schedule(\n",
        "#     name=\"rss-pipeline-schedule\",\n",
        "#     description=\"Runs the pipeline every day at 1:00 AM\",\n",
        "#     pipeline_job=pipeline_job,\n",
        "#     recurrence={\n",
        "#         \"frequency\": \"Day\",\n",
        "#         \"interval\": 1,\n",
        "#         \"start_time\": datetime.utcnow() + timedelta(days=1),  # Start tomorrow\n",
        "#         \"hours\": [1]  # Run at 1:00 AM UTC\n",
        "#     },\n",
        "#     experiment_name=\"rss-crawl-exp-1\",  # Replace with your experiment name\n",
        "#     wait_for_provisioning=True,\n",
        "#     wait_timeout=300\n",
        "# )\n",
        "\n",
        "# # Create the schedule job\n",
        "# try:\n",
        "#     submitted_schedule_job = ml_client.jobs.create_or_update(\n",
        "#         schedule\n",
        "#     )\n",
        "#     print(f\"Schedule submitted successfully with name: {submitted_schedule_job.name}\")\n",
        "\n",
        "# except Exception as e:\n",
        "#     print(f\"Error creating schedule job: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Warnings: [jobs.rss_fetch_feed_job.allow_reuse: Unknown field.]\nWarnings: [jobs.rss_fetch_feed_job.allow_reuse: Unknown field.]\npathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: sad_jicama_xvp85h3td3\nWeb View: https://ml.azure.com/runs/sad_jicama_xvp85h3td3?wsid=/subscriptions/541beb67-718e-41c5-958e-8cc0ba95b210/resourcegroups/awesome_rag_dev/workspaces/rag_book_demo\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-09-02 09:57:52Z] Submitting 1 runs, first five are: be9a0e54:64249c73-e7ab-4389-ac02-c45cf0ab0a2d\n"
        }
      ],
      "execution_count": 159,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725270183821
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the schedule\n",
        "# schedule = Schedule(\n",
        "#     name=\"rss_crawl_schedule\",\n",
        "#     description=\"Runs the pipeline every day at 2AM\",\n",
        "#     recurrence={\"frequency\": \"day\", \"interval\": 1},\n",
        "#     pipeline_job=pipeline_job,\n",
        "#     plan={\"plan_type\": \"fixed_schedule\", \"plan_details\": {\"time_of_day\": \"02:00\"}}\n",
        "# )\n",
        "\n",
        "# # Create the schedule\n",
        "# ml_client.schedules.create_or_update(schedule)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725270183929
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the schedule parameters\n",
        "# schedule_params = {\n",
        "#     \"name\": \"rss-pipeline-schedule\",\n",
        "#     \"description\": \"Runs the pipeline every day at 1:00 AM\",\n",
        "#     \"recurrence\": {\n",
        "#         \"frequency\": \"Day\",\n",
        "#         \"interval\": 1,\n",
        "#         \"start_time\": datetime.utcnow() + timedelta(days=1),  # Start tomorrow\n",
        "#         \"hours\": [1]  # Run at 1:00 AM UTC\n",
        "#     },\n",
        "#     \"experiment_name\": \"rss-crawl-exp-1\",  # Replace with your experiment name\n",
        "#     \"wait_for_provisioning\": True,\n",
        "#     \"wait_timeout\": 300\n",
        "# }\n",
        "\n",
        "\n",
        "# # Create the scheduled job\n",
        "# try:\n",
        "#     submitted_job = ml_client.jobs.create_or_update(\n",
        "#         pipeline_job,\n",
        "#         experiment_name=schedule_params[\"experiment_name\"],\n",
        "#         schedule=schedule_params\n",
        "#     )\n",
        "#     print(f\"Scheduled job submitted successfully with name: {submitted_job.name}\")\n",
        "\n",
        "# except Exception as e:\n",
        "#     print(f\"Error creating scheduled job: {e}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725270183941
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from azure.ai.ml.entities import Schedule, RecurrenceTrigger, RecurrencePattern\n",
        "\n",
        "# # Define the recurrence trigger\n",
        "# recurrence_trigger = RecurrenceTrigger(\n",
        "#     frequency=\"Day\",\n",
        "#     interval=1,\n",
        "#     start_time=(datetime.utcnow() + timedelta(days=1)).isoformat(),\n",
        "#     time_zone=\"UTC\",\n",
        "#     schedule=RecurrencePattern(hours=[1])\n",
        "# )\n",
        "\n",
        "# schedule = Schedule(\n",
        "#     name=schedule_params[\"name\"],\n",
        "#     description=schedule_params[\"description\"],\n",
        "#     trigger=recurrence_trigger,\n",
        "#     create_job=pipeline_job\n",
        "# )\n",
        "\n",
        "# submitted_schedule = ml_client.schedules.create_or_update(schedule)\n",
        "# print(f\"Scheduled job submitted successfully with name: {submitted_schedule.name}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725270183953
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.19",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
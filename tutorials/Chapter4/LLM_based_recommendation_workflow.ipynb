{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting for Personalized Book Recommendations with Workflow\n",
        "\n",
        "**Objective:**\n",
        "\n",
        "This tutorial combines effective prompting techniques with an end-to-end workflow for personalized book recommendations. You will learn how to design a recommendation system using an LLM that recommends books based on a user's preferences.\n",
        "\n",
        "#### End-to-End Workflow Design\n",
        "\n",
        "The workflow for personalized book recommendations includes:\n",
        "\n",
        "- Input Data Preparation: Collect user preferences and book metadata.\n",
        "- Prompt Construction: Design prompts to elicit recommendations.\n",
        "- LLM Inference: Use an LLM to generate recommendations.\n",
        "- Post-Processing: Refine results using additional filtering or embeddings.\n",
        "- Evaluation: Assess the relevance and diversity of recommendations.\n",
        "#### Set Up the Azure ML Environment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Azure ML SDK if not already installed\n",
        "pip install azureml-core azureml-pipeline"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define Data Preparation Step\n",
        "\n",
        "Process user profile and book metadata."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/prep_step.py\n",
        "import argparse\n",
        "import pandas as pd\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--user_profile\", type=str, help=\"Path to user profile TSV\")\n",
        "    parser.add_argument(\"--book_metadata\", type=str, help=\"Path to book metadata CSV\")\n",
        "    parser.add_argument(\"--output\", type=str, help=\"Path to save processed data\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Load and process data\n",
        "    user_profile = pd.read_csv(args.user_profile, sep='\\t')\n",
        "    book_metadata = pd.read_csv(args.book_metadata)\n",
        "\n",
        "    # Save preprocessed data\n",
        "    output_path = args.output + \"/processed_data.csv\"\n",
        "    book_metadata.to_csv(output_path, index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define LLM Inference Step\n",
        "\n",
        "Generate recommendations using prompts.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "from langchain import OpenAI\n",
        "\n",
        "# Step 1: LLM-based Tokenization\n",
        "def tokenize_user_item_interactions(data):\n",
        "    \"\"\"\n",
        "    Tokenize user-item interactions for LLM-based recommendation systems.\n",
        "    \n",
        "    Args:\n",
        "        data: A DataFrame with 'user_id', 'item_id', 'interaction_type', 'timestamp'.\n",
        "        \n",
        "    Returns:\n",
        "        A DataFrame with a new 'tokenized_sequence' column.\n",
        "    \"\"\"\n",
        "    def tokenize_row(row):\n",
        "        # Mapping the user and item to tokens\n",
        "        user_token = f\"USER_{row['user_id']}\"\n",
        "        item_token = f\"ITEM_{row['item_id']}\"\n",
        "        \n",
        "        # Encoding the interaction type and timestamp to tokens\n",
        "        interaction_token = row['interaction_type'].upper()  # CLICK, PURCHASE, etc.\n",
        "        time_token = f\"TIME_{row['timestamp'].split(' ')[1].upper()}\"  # TIME_MORNING, TIME_EVENING\n",
        "        \n",
        "        # Combine into a sequence for the LLM\n",
        "        return f\"[{user_token}, {interaction_token}, {item_token}, {time_token}]\"\n",
        "\n",
        "    # Apply the tokenization to each row of the DataFrame\n",
        "    data['tokenized_sequence'] = data.apply(tokenize_row, axis=1)\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Step 2: Prompt-Based Recommendation using LLM\n",
        "def generate_recommendations(data, llm_model):\n",
        "    \"\"\"\n",
        "    Generate recommendations for each user-item interaction using the LLM.\n",
        "    \n",
        "    Args:\n",
        "        data: A DataFrame with a 'tokenized_sequence' column.\n",
        "        llm_model: A pre-initialized LLM model (e.g., OpenAI's GPT).\n",
        "        \n",
        "    Returns:\n",
        "        A DataFrame with a 'recommendations' column.\n",
        "    \"\"\"\n",
        "    def get_recommendation(sequence):\n",
        "        prompt = f\"Given the user-item interaction: {sequence}, recommend similar items.\"\n",
        "        response = llm_model(prompt)\n",
        "        return response.strip()  # Clean the response\n",
        "\n",
        "    # Apply the LLM inference for recommendations\n",
        "    data['recommendations'] = data['tokenized_sequence'].apply(lambda x: get_recommendation(x))\n",
        "    \n",
        "    return data\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--input\", type=str, help=\"Path to processed data (CSV format)\")\n",
        "    parser.add_argument(\"--output\", type=str, help=\"Path to save recommendations (CSV format)\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Load processed user-item interaction data\n",
        "    data = pd.read_csv(args.input)\n",
        "    \n",
        "    # Step 1: LLM-based Tokenization\n",
        "    data = tokenize_user_item_interactions(data)\n",
        "    \n",
        "    # Initialize the LLM (GPT-4 example here)\n",
        "    llm = OpenAI(model=\"gpt-4\", max_tokens=150)\n",
        "    \n",
        "    # Step 2: Generate recommendations using LLM\n",
        "    data = generate_recommendations(data, llm)\n",
        "    \n",
        "    # Save the resulting DataFrame with recommendations\n",
        "    data.to_csv(args.output + \"/recommendations.csv\", index=False)\n",
        "    print(\"Recommendations saved to\", args.output + \"/recommendations.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define Evaluation Step\n",
        "\n",
        "Assess the relevance of recommendations."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/evaluation_step.py\n",
        "import argparse\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import pandas as pd\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--input\", type=str, help=\"Path to recommendations\")\n",
        "    parser.add_argument(\"--output\", type=str, help=\"Path to save evaluation results\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Load recommendations\n",
        "    data = pd.read_csv(args.input)\n",
        "\n",
        "    # Evaluate using embeddings\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    data['scores'] = data['recommendations'].apply(lambda x: util.cos_sim(model.encode(x), model.encode(\"science fiction, space exploration\")))\n",
        "\n",
        "    # Save evaluation results\n",
        "    data.to_csv(args.output + \"/evaluation_results.csv\", index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Azure ML Pipeline with Script Files\n",
        "**Define the Pipeline Steps**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Experiment, Environment\n",
        "from azureml.pipeline.core import Pipeline, PipelineData\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.core.compute import ComputeTarget\n",
        "\n",
        "# Set up Azure ML workspace and environment\n",
        "ws = Workspace.from_config()\n",
        "compute_target = ComputeTarget(workspace=ws, name=\"your-compute-cluster\")\n",
        "env = Environment.from_conda_specification(name=\"pipeline-env\", file_path=\"environment.yml\")\n",
        "\n",
        "# Define data outputs\n",
        "prep_output = PipelineData(\"prep_output\", datastore=ws.get_default_datastore())\n",
        "inference_output = PipelineData(\"inference_output\", datastore=ws.get_default_datastore())\n",
        "evaluation_output = PipelineData(\"evaluation_output\", datastore=ws.get_default_datastore())\n",
        "\n",
        "# Step 1: Data Preparation\n",
        "prep_step = PythonScriptStep(\n",
        "    name=\"Data Preparation\",\n",
        "    script_name=\"scripts/prep_step.py\",\n",
        "    arguments=[\"--user_profile\", \"user_profile.tsv\", \"--book_metadata\", \"book_metadata.csv\", \"--output\", prep_output],\n",
        "    outputs=[prep_output],\n",
        "    compute_target=compute_target,\n",
        "    runconfig=env,\n",
        "    allow_reuse=True,\n",
        ")\n",
        "\n",
        "# Step 2: LLM Inference\n",
        "inference_step = PythonScriptStep(\n",
        "    name=\"LLM Inference\",\n",
        "    script_name=\"scripts/inference_step.py\",\n",
        "    arguments=[\"--input\", prep_output, \"--output\", inference_output],\n",
        "    inputs=[prep_output],\n",
        "    outputs=[inference_output],\n",
        "    compute_target=compute_target,\n",
        "    runconfig=env,\n",
        "    allow_reuse=True,\n",
        ")\n",
        "\n",
        "# Step 3: Evaluation\n",
        "evaluation_step = PythonScriptStep(\n",
        "    name=\"Evaluation\",\n",
        "    script_name=\"scripts/evaluation_step.py\",\n",
        "    arguments=[\"--input\", inference_output, \"--output\", evaluation_output],\n",
        "    inputs=[inference_output],\n",
        "    outputs=[evaluation_output],\n",
        "    compute_target=compute_target,\n",
        "    runconfig=env,\n",
        "    allow_reuse=True,\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Combine Steps into a Pipeline**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline\n",
        "pipeline = Pipeline(workspace=ws, steps=[prep_step, inference_step, evaluation_step])\n",
        "pipeline.validate()\n",
        "\n",
        "# Submit pipeline\n",
        "experiment = Experiment(workspace=ws, name=\"book-recommendation-pipeline\")\n",
        "pipeline_run = experiment.submit(pipeline)\n",
        "pipeline_run.wait_for_completion(show_output=True)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
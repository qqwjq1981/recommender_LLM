{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning an LLM for Movie Recommendations\n",
        "\n",
        "#### Goals\n",
        "- Improve the model's relevance in generating recommendations.\n",
        "- Address challenges of domain-specific language and sparse user data.\n",
        "- Create a deployable recommendation pipeline.\n",
        "\n",
        "#### Requirements\n",
        "\n",
        "- A pre-trained LLM (e.g., GPT-3, GPT-4, or an open-source model like GPT-J).\n",
        "- A fine-tuning framework such as Hugging Face Transformers.\n",
        "- A movie dataset (e.g., MovieLens or IMDb).\n",
        "- Compute resources (GPU recommended)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets accelerate peft promptify"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Prepare the dataset\n",
        "\n",
        "Use a dataset containing:\n",
        "\n",
        "- Movie descriptions (e.g., summaries, genres, metadata).\n",
        "- User-item interaction data (e.g., ratings, reviews, watch history).\n",
        "\n",
        "Input-Output Pair Formatting:\n",
        "\n",
        "- Input: A prompt with user context, such as:\n",
        "\n",
        "`User preferences: [list of liked movies]. Recommend 3 movies based on their taste.`\n",
        "\n",
        "- Output: A list of relevant recommendations or a response explaining the recommendations."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load MovieLens data\n",
        "ratings = pd.read_csv(\"https://files.grouplens.org/datasets/movielens/ml-latest-small/ratings.csv\")\n",
        "movies = pd.read_csv(\"https://files.grouplens.org/datasets/movielens/ml-latest-small/movies.csv\")\n",
        "\n",
        "# Merge ratings with movie metadata\n",
        "data = ratings.merge(movies, on='movieId')\n",
        "data.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Preprocess the Data\n",
        "\n",
        "- Clean and deduplicate entries.\n",
        "- Tokenize using a tokenizer matching the pre-trained LLM."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt-4\")\n",
        "data[\"input\"] = data[\"user_preferences\"] + data[\"movie_metadata\"]\n",
        "data[\"input\"] = data[\"input\"].apply(lambda x: tokenizer.encode(x, truncation=True, padding=\"max_length\"))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Format Input-Output Pairs\n",
        "\n",
        "Create a prompt-response dataset for fine-tuning."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Format prompts and responses\n",
        "def create_prompt_response(group):\n",
        "    movies = list(group['title'])\n",
        "    liked_movies = random.sample(movies, min(len(movies), 3))  # Randomly pick 3 liked movies\n",
        "    prompt = f\"User preferences: {', '.join(liked_movies)}. Recommend 3 movies based on their taste.\"\n",
        "    recommendations = random.sample(movies, min(len(movies), 3))  # Placeholder for recommendations\n",
        "    return pd.Series({'prompt': prompt, 'response': recommendations})\n",
        "\n",
        "formatted_data = data.groupby('userId').apply(create_prompt_response).reset_index(drop=True)\n",
        "formatted_data.head()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Fine-Tune Methods\n",
        "\n",
        "** a. Full Fine-Tuning **"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['prompt'], text_target=batch['response'], truncation=True)\n",
        "\n",
        "dataset = formatted_data.map(tokenize, batched=True)\n",
        "dataset.set_format(\"torch\")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./full_finetune\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    save_total_limit=1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** b. LoRA (Parameter-Efficient Fine-Tuning) **\n",
        "\n",
        "LoRA requires fewer resources by updating a subset of parameters.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** c. Prompt Tuning ** Prompt tuning focuses on learning optimal prompts for task-specific responses.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PromptTuningConfig\n",
        "\n",
        "prompt_config = PromptTuningConfig(model_name_or_path=model_name, task_type=\"causal-lm\")\n",
        "prompt_tuning_model = get_peft_model(model, prompt_config)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 5: Evaluate Dataset Diversity\n",
        "\n",
        "Split data into diverse and limited subsets and compare performance."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset: Limited user preferences\n",
        "limited_data = formatted_data.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# Fine-tune and evaluate models with diverse vs. limited data\n",
        "diverse_results = trainer.evaluate(eval_dataset=dataset['test'])\n",
        "limited_results = trainer.evaluate(eval_dataset=limited_data)\n",
        "print(f\"Diverse Data Results: {diverse_results}\")\n",
        "print(f\"Limited Data Results: {limited_results}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 6: Analyze and Compare\n",
        "\n",
        "Compare methods using metrics like precision@K, recall@K."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def precision_at_k(recommended, relevant, k):\n",
        "    \"\"\"Calculate Precision@K.\"\"\"\n",
        "    recommended_at_k = recommended[:k]\n",
        "    relevant_set = set(relevant)\n",
        "    precision = len(set(recommended_at_k) & relevant_set) / len(recommended_at_k)\n",
        "    return precision\n",
        "\n",
        "def recall_at_k(recommended, relevant, k):\n",
        "    \"\"\"Calculate Recall@K.\"\"\"\n",
        "    recommended_at_k = recommended[:k]\n",
        "    relevant_set = set(relevant)\n",
        "    recall = len(set(recommended_at_k) & relevant_set) / len(relevant_set)\n",
        "    return recall\n",
        "\n",
        "def evaluate_model(model, tokenizer, test_dataset, k_values=[3, 5, 10]):\n",
        "    \"\"\"Evaluate a model using Precision@K and Recall@K.\"\"\"\n",
        "    precision_scores = {k: [] for k in k_values}\n",
        "    recall_scores = {k: [] for k in k_values}\n",
        "\n",
        "    for sample in test_dataset:\n",
        "        prompt = sample['prompt']\n",
        "        ground_truth = sample['response'].split(\", \")  # Relevant items\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "        recommended = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\", \")\n",
        "\n",
        "        for k in k_values:\n",
        "            precision_scores[k].append(precision_at_k(recommended, ground_truth, k))\n",
        "            recall_scores[k].append(recall_at_k(recommended, ground_truth, k))\n",
        "\n",
        "    avg_precision = {k: np.mean(precision_scores[k]) for k in k_values}\n",
        "    avg_recall = {k: np.mean(recall_scores[k]) for k in k_values}\n",
        "    return avg_precision, avg_recall\n",
        "\n",
        "# Define K values\n",
        "k_values = [3, 5, 10]\n",
        "\n",
        "# Evaluate Full Fine-Tuning\n",
        "full_finetune_precision, full_finetune_recall = evaluate_model(model, tokenizer, dataset['test'], k_values)\n",
        "\n",
        "# Evaluate LoRA\n",
        "lora_precision, lora_recall = evaluate_model(peft_model, tokenizer, dataset['test'], k_values)\n",
        "\n",
        "# Evaluate Prompt Tuning\n",
        "prompt_tuning_precision, prompt_tuning_recall = evaluate_model(prompt_tuning_model, tokenizer, dataset['test'], k_values)\n",
        "\n",
        "# Aggregate Results\n",
        "results = []\n",
        "for k in k_values:\n",
        "    results.append({\n",
        "        \"Method\": \"Full Fine-Tuning\",\n",
        "        f\"Precision@{k}\": full_finetune_precision[k],\n",
        "        f\"Recall@{k}\": full_finetune_recall[k],\n",
        "    })\n",
        "    results.append({\n",
        "        \"Method\": \"LoRA\",\n",
        "        f\"Precision@{k}\": lora_precision[k],\n",
        "        f\"Recall@{k}\": lora_recall[k],\n",
        "    })\n",
        "    results.append({\n",
        "        \"Method\": \"Prompt Tuning\",\n",
        "        f\"Precision@{k}\": prompt_tuning_precision[k],\n",
        "        f\"Recall@{k}\": prompt_tuning_recall[k],\n",
        "    })\n",
        "\n",
        "# Convert Results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.19",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
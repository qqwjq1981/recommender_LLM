{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Ablation Study for multi-modal integration\n",
        "\n",
        "The tutorial compares three approaches:\n",
        "\n",
        "- **Multi-modal integration:** Combines text and image embeddings.\n",
        "- **Multi-modal with reduced images:** Removes some image embeddings.\n",
        "- **Image-to-text conversion with text-only methods:** Converts images to textual descriptions and uses text embeddings only.\n",
        "\n",
        "#### Setup\n",
        "\n",
        "Dataset: Use a dataset with text and images (e.g., MS-COCO or a custom dataset). The dataset should have the following:\n",
        "\n",
        "- Text: Descriptions.\n",
        "- Images: Associated images."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy pandas sklearn transformers sentence-transformers torch torchvision PIL"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Load models and dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Models\n",
        "text_model = SentenceTransformer('all-MiniLM-L6-v2')  # Text embeddings\n",
        "image_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "image_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Load Dataset\n",
        "# Assume a dataset with 'text', 'image_path', and 'label' columns\n",
        "data = pd.DataFrame({\n",
        "    'text': [\"A dog in a park\", \"A sunny beach\", \"A plate of food\"],\n",
        "    'image_path': [\"dog.jpg\", \"beach.jpg\", \"food.jpg\"],\n",
        "    'label': [1, 2, 3]  # Dummy labels\n",
        "})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Define Functions for Ablation and embeddings\n",
        "You will create functions to perform the ablation by removing images from the embeddings and then evaluate retrieval performance."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Utility Functions\n",
        "def compute_text_embedding(texts):\n",
        "    \"\"\"Compute text embeddings using SentenceTransformer.\"\"\"\n",
        "    return text_model.encode(texts, convert_to_tensor=True)\n",
        "\n",
        "def compute_image_embedding(image_paths):\n",
        "    \"\"\"Compute image embeddings using CLIP.\"\"\"\n",
        "    images = [Image.open(img_path).convert(\"RGB\") for img_path in image_paths]\n",
        "    inputs = image_processor(images=images, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        return image_model.get_image_features(**inputs).cpu()\n",
        "\n",
        "def evaluate_retrieval(query_embeddings, candidate_embeddings, true_labels, k=5):\n",
        "    \"\"\"Evaluate retrieval metrics: precision@k and recall@k.\"\"\"\n",
        "    similarities = cosine_similarity(query_embeddings, candidate_embeddings)\n",
        "    top_k_indices = np.argsort(similarities, axis=1)[:, -k:]\n",
        "    precision = np.mean([\n",
        "        precision_score([label], top_k_indices[i], average='micro') \n",
        "        for i, label in enumerate(true_labels)\n",
        "    ])\n",
        "    recall = np.mean([\n",
        "        recall_score([label], top_k_indices[i], average='micro') \n",
        "        for i, label in enumerate(true_labels)\n",
        "    ])\n",
        "    return precision, recall\n",
        "\n",
        "# Compute Embeddings\n",
        "text_embeddings = compute_text_embedding(data['text'].tolist())\n",
        "image_embeddings = compute_image_embedding(data['image_path'].tolist())\n",
        "\n",
        "# Combine text and image embeddings for multi-modal integration\n",
        "multi_modal_embeddings = np.concatenate([text_embeddings, image_embeddings], axis=1)\n",
        "\n",
        "# Ablation Functions\n",
        "def remove_images(embeddings, fraction):\n",
        "    \"\"\"Remove a fraction of image embeddings by setting them to zeros.\"\"\"\n",
        "    num_remove = int(fraction * embeddings.shape[0])\n",
        "    embeddings[:num_remove, text_embeddings.shape[1]:] = 0\n",
        "    return embeddings\n",
        "\n",
        "def convert_images_to_text(image_paths):\n",
        "    \"\"\"Convert images to text using a dummy function or pre-trained model.\"\"\"\n",
        "    # Dummy conversion (real case would use image captioning)\n",
        "    return [\"Generated text for image \" + str(i) for i, _ in enumerate(image_paths)]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Run the Ablation Study\n",
        "\n",
        "Now, you can run your ablation study by varying the fraction of images removed and evaluating the impact on retrieval metrics."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Case 1: Multi-modal integration\n",
        "precision_mm, recall_mm = evaluate_retrieval(multi_modal_embeddings, multi_modal_embeddings, data['label'].values)\n",
        "\n",
        "# Case 2: Multi-modal with reduced images\n",
        "reduced_embeddings = remove_images(multi_modal_embeddings.copy(), fraction=0.5)\n",
        "precision_reduced, recall_reduced = evaluate_retrieval(reduced_embeddings, reduced_embeddings, data['label'].values)\n",
        "\n",
        "# Case 3: Convert images to text\n",
        "image_texts = convert_images_to_text(data['image_path'].tolist())\n",
        "image_text_embeddings = compute_text_embedding(image_texts)\n",
        "text_only_embeddings = np.concatenate([text_embeddings, image_text_embeddings], axis=1)\n",
        "precision_text_only, recall_text_only = evaluate_retrieval(text_only_embeddings, text_only_embeddings, data['label'].values)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Present the results"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Results\n",
        "print(\"Multi-modal Integration: Precision@k:\", precision_mm, \"Recall@k:\", recall_mm)\n",
        "print(\"Reduced Images: Precision@k:\", precision_reduced, \"Recall@k:\", recall_reduced)\n",
        "print(\"Text-Only (Images to Text): Precision@k:\", precision_text_only, \"Recall@k:\", recall_text_only)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
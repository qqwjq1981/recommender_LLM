{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Generative Recommendation with Multi-Modal Data Integration\n",
        "\n",
        "#### Objective\n",
        "- Generate personalized product recommendations using generative models.\n",
        "- Integrate multi-modal data (product descriptions and images) to understand product features better.\n",
        "- Use multi-modal embeddings to represent text and image data jointly, enabling more context-aware suggestions.\n",
        "\n",
        "#### Dataset\n",
        "We will use a dataset containing:\n",
        "\n",
        "- User-item interactions: Clicks, purchases, and ratings.\n",
        "- Product descriptions: Textual data describing each product.\n",
        "- Product images: Visual content associated with products.\n",
        "\n",
        "#### Required Libraries\n",
        "- Python\n",
        "- TensorFlow or PyTorch\n",
        "- Hugging Face Transformers (for text embeddings)\n",
        "- OpenCV or Pillow (for image processing)\n",
        "- Pre-trained models (e.g., BERT, ResNet)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Preprocessing and Embedding Creation\n",
        "\n",
        "**Product Descriptions (Text)**\n",
        "\n",
        "Tokenize and preprocess product descriptions using a pre-trained BERT model to generate text embeddings."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def get_text_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).detach()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Product Images (Visual Data)**\n",
        "\n",
        "Generate embeddings for product images using a pre-trained ResNet model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Load pre-trained ResNet model\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "def get_image_embedding(image_path):\n",
        "    img = Image.open(image_path)\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    img_tensor = preprocess(img).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        embedding = model(img_tensor)\n",
        "    return embedding\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combining Text and Image Data**\n",
        "\n",
        "Create a unified representation by concatenating the embeddings from text and image data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def combine_embeddings(text_embeddings, image_embeddings):\n",
        "    return [torch.cat((text_embed, img_embed), dim=1) \n",
        "            for text_embed, img_embed in zip(text_embeddings, image_embeddings)]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generating Personalized Recommendations\n",
        "\n",
        "**Using VAEs for Recommendations**\n",
        "\n",
        "Train a VAE on combined embeddings to model user preferences and generate recommendations."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        # Encoder\n",
        "        self.fc1 = nn.Linear(input_dim, 512)\n",
        "        self.fc2_mu = nn.Linear(512, latent_dim)\n",
        "        self.fc2_logvar = nn.Linear(512, latent_dim)\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(latent_dim, 512)\n",
        "        self.fc4 = nn.Linear(512, input_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = torch.relu(self.fc1(x))\n",
        "        return self.fc2_mu(h1), self.fc2_logvar(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = torch.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation and Testing\n",
        "\n",
        "** Performance Metrics **\n",
        "\n",
        "Evaluate recommendations using metrics like Precision@k, Recall@k, and Normalized Discounted Cumulative Gain (NDCG)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(recommended, relevant, k):\n",
        "    recommended_at_k = recommended[:k]\n",
        "    return len(set(recommended_at_k) & set(relevant)) / k\n",
        "\n",
        "def recall_at_k(recommended, relevant, k):\n",
        "    recommended_at_k = recommended[:k]\n",
        "    return len(set(recommended_at_k) & set(relevant)) / len(relevant)\n",
        "\n",
        "def ndcg_at_k(recommended, relevant, k):\n",
        "    dcg = sum([1 / (math.log2(i + 2)) for i, r in enumerate(recommended[:k]) if r in relevant])\n",
        "    idcg = sum([1 / (math.log2(i + 2)) for i in range(min(len(relevant), k))])\n",
        "    return dcg / idcg\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main function"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load data and preprocess\n",
        "    product_descriptions = [\"Elegant watch\", \"Stylish sunglasses\"]\n",
        "    product_images = [\"watch.jpg\", \"sunglasses.jpg\"]\n",
        "    \n",
        "    text_embeddings = [get_text_embedding(desc) for desc in product_descriptions]\n",
        "    image_embeddings = [get_image_embedding(img) for img in product_images]\n",
        "    \n",
        "    combined_embeddings = combine_embeddings(text_embeddings, image_embeddings)\n",
        "    \n",
        "    # Train VAE\n",
        "    vae = VAE(input_dim=combined_embeddings[0].shape[1], latent_dim=32)\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
        "    for epoch in range(10):  # Simplified training loop\n",
        "        for data in combined_embeddings:\n",
        "            recon, mu, logvar = vae(data)\n",
        "            loss = loss_function(recon, data, mu, logvar)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    \n",
        "    # Evaluate model\n",
        "    recommended = [\"watch\", \"sunglasses\"]\n",
        "    relevant = [\"watch\"]\n",
        "    print(f\"Precision@1: {precision_at_k(recommended, relevant, 1)}\")\n",
        "    print(f\"Recall@1: {recall_at_k(recommended, relevant, 1)}\")\n",
        "    print(f\"NDCG@1: {ndcg_at_k(recommended, relevant, 1)}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}